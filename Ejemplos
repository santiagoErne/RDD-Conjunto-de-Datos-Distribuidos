from pyspark.context import SparkContext
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
# Inicializar SparkContext
ejemplo = SparkContext.getOrCreate()

#map
rdd = ejemplo.parallelize(["b", "a", "c"])
sorted(rdd.map(lambda x: (x, 1)).collect())

[('a', 1), ('b', 1), ('c', 1)]

# Creamos un RDD con una lista de números
rdd = ejemplo.parallelize([1, 2, 3, 4, 5])

# Usamos map para elevar cada número al cuadrado
resultado = rdd.map(lambda x: x ** 2).collect()

print(resultado)

[1, 4, 9, 16, 25]

#filter
rdd = ejemplo.parallelize([1, 2, 3, 4, 5])
rdd.filter(lambda x: x % 2 == 0).collect()

[2, 4]

# Creamos un RDD con una lista de números
rdd = ejemplo.parallelize([10, 15, 20, 25, 30, 35])

# Usamos filter para quedarnos solo con los números mayores a 20
resultado = rdd.filter(lambda x: x > 20).collect()

print(resultado)

[25, 30, 35]

#flatMap
rdd = ejemplo.parallelize([2, 3, 4])
sorted(rdd.flatMap(lambda x: range(1, x)).collect())
[1, 1, 1, 2, 2, 3]
sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())

[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]

# Creamos un RDD con una lista de frases
rdd = ejemplo.parallelize(["Hola mundo", "aprendiendo Spark", "con flatMap"])
# Usamos flatMap para dividir cada frase en palabras
resultado = rdd.flatMap(lambda frase: frase.split(" ")).collect()
print(resultado)

['Hola', 'mundo', 'aprendiendo', 'Spark', 'con', 'flatMap']

#union
rdd = ejemplo.parallelize([1, 1, 2, 3])
rdd.union(rdd).collect()

[1, 1, 2, 3, 1, 1, 2, 3]

# Creamos dos RDDs con listas de números
rdd1 = ejemplo.parallelize([1, 2, 3])
rdd2 = ejemplo.parallelize([4, 5, 6])

# Usamos union para combinar los dos RDDs
resultado = rdd1.union(rdd2).collect()

print(resultado)

[1, 2, 3, 4, 5, 6]

#intersection
rdd1 = ejemplo.parallelize([1, 10, 2, 3, 4, 5])
rdd2 = ejemplo.parallelize([1, 6, 2, 3, 7, 8])
rdd1.intersection(rdd2).collect()

[1, 2, 3]

# Creamos dos RDDs con algunos números en común
rdd1 = ejemplo.parallelize([1, 2, 3, 4])
rdd2 = ejemplo.parallelize([3, 4, 5, 6])

# Usamos intersection para obtener los elementos comunes entre los dos RDDs
resultado = rdd1.intersection(rdd2).collect()

print(resultado)

[4, 3]

#distinct
sorted(ejemplo.parallelize([1, 1, 2, 3]).distinct().collect())

[1, 2, 3]

# Creamos un RDD con algunos números repetidos
rdd = ejemplo.parallelize([1, 2, 2, 3, 4, 4, 5])

# Usamos distinct para eliminar los elementos duplicados
resultado = rdd.distinct().collect()

print(resultado)

[1, 2, 3, 4, 5]

#groupByKey
rdd = ejemplo.parallelize([("a", 1), ("b", 1), ("a", 1)])
sorted(rdd.groupByKey().mapValues(len).collect())

[('a', 2), ('b', 1)]

sorted(rdd.groupByKey().mapValues(list).collect())

[('a', [1, 1]), ('b', [1])]

# Creamos un RDD de pares clave-valor
rdd = ejemplo.parallelize([("a", 1), ("b", 2), ("a", 3), ("b", 4), ("c", 5)])

# Usamos groupByKey para agrupar los valores por cada clave
resultado = rdd.groupByKey().mapValues(list).collect()

print(resultado)

[('b', [2, 4]), ('c', [5]), ('a', [1, 3])]
