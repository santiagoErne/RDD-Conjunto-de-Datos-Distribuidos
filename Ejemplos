from pyspark.context import SparkContext
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
# Inicializar SparkContext
ejemplo = SparkContext.getOrCreate()

#map
rdd = ejemplo.parallelize(["b", "a", "c"])
sorted(rdd.map(lambda x: (x, 1)).collect())

[('a', 1), ('b', 1), ('c', 1)]

#filter
rdd = ejemplo.parallelize([1, 2, 3, 4, 5])
rdd.filter(lambda x: x % 2 == 0).collect()

[2, 4]

#flatMap
rdd = ejemplo.parallelize([2, 3, 4])
sorted(rdd.flatMap(lambda x: range(1, x)).collect())
[1, 1, 1, 2, 2, 3]
sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())

[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]

# Creamos un RDD con una lista de frases
rdd = ejemplo.parallelize(["Hola mundo", "aprendiendo Spark", "con flatMap"])
# Usamos flatMap para dividir cada frase en palabras
resultado = rdd.flatMap(lambda frase: frase.split(" ")).collect()
print(resultado)

['Hola', 'mundo', 'aprendiendo', 'Spark', 'con', 'flatMap']

#union
rdd = ejemplo.parallelize([1, 1, 2, 3])
rdd.union(rdd).collect()

[1, 1, 2, 3, 1, 1, 2, 3]

#intersection
rdd1 = ejemplo.parallelize([1, 10, 2, 3, 4, 5])
rdd2 = ejemplo.parallelize([1, 6, 2, 3, 7, 8])
rdd1.intersection(rdd2).collect()

[1, 2, 3]

#distinct
sorted(ejemplo.parallelize([1, 1, 2, 3]).distinct().collect())

[1, 2, 3]

#groupByKey
rdd = ejemplo.parallelize([("a", 1), ("b", 1), ("a", 1)])
sorted(rdd.groupByKey().mapValues(len).collect())

[('a', 2), ('b', 1)]

sorted(rdd.groupByKey().mapValues(list).collect())

[('a', [1, 1]), ('b', [1])]

